package processing


import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.{col, explode}

object Tweets {

  def main(args: Array[String]): Unit = {

    val logger = Logger.getLogger("org.apache.spark")
    logger.setLevel(Level.ERROR)

    val spark = SparkSession
      .builder()
      .appName("Processing_api")
      //.master("local[*]")
      .getOrCreate()

    val data = spark.read.json("hdfs:///user/acerrato/bicimad_m3/data/streaming/*")
    ///val data = spark.read.text("./src/main/scala/resources/usage-.1553461984272")

    //transformamos el JSON para extraer los campos
    val BM = data.select("time", "_station")
    val bicimad = BM.withColumn("time", explode(col("_station")))

    val def_bicimad = bicimad.select("time", "_station.id", "_station.light")
    def_bicimad.show()
  }

}
